# PhilGEPS Data Pipeline Documentation

This document provides a comprehensive overview of the complete data pipeline, from raw data sources through all processing steps to final outputs and their usage in the codebase.

## ğŸ“Š **Data Pipeline Overview**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                RAW DATA SOURCES                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2013-2020 XLSX Files          â”‚ 2021-2025 CSV Files          â”‚ Flood Control   â”‚
â”‚ data/raw/PHILGEPS 2013-2021/  â”‚ data/raw/PHILGEPS -- 2021-2025/ â”‚ data/raw/SSP/ â”‚
â”‚ ~8.9M records, 44 columns     â”‚ ~2.2M records, 47 columns    â”‚ Specialized     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            DATA PROCESSING SCRIPTS                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ process_xlsx_to_parquet_duckdb_basic_only.py  â”‚ rebuild_step_by_step.py        â”‚
â”‚ â€¢ Convert XLSX to Parquet                     â”‚ â€¢ Complete data rebuild        â”‚
â”‚ â€¢ Standardize headers (row 3)                 â”‚ â€¢ Process both XLSX + CSV      â”‚
â”‚ â€¢ Deduplication & cleaning                    â”‚ â€¢ Combine datasets             â”‚
â”‚ â€¢ Generate unique reference IDs               â”‚ â€¢ Add system fields            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              INTERMEDIATE FILES                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Quarterly Raw Files (data/processed/*_Q*_contracts.parquet)                    â”‚
â”‚ â€¢ 36 quarterly files, 44 columns each                                         â”‚
â”‚ â€¢ ~1.2GB total size                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              CONSOLIDATED DATA                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ all_contracts_consolidated.parquet                                             â”‚
â”‚ â€¢ 6.5M records, 50 columns                                                    â”‚
â”‚ â€¢ 2.8GB file size                                                             â”‚
â”‚ â€¢ Complete 2013-2025 dataset                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            DATA QUALITY & CLEANING                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ fix_date_formats.py                    â”‚ generate_clean_awarded_contracts.py   â”‚
â”‚ â€¢ Fix Excel serial dates               â”‚ â€¢ Filter awarded contracts only       â”‚
â”‚ â€¢ Convert to proper date format        â”‚ â€¢ contract_amount > 0                â”‚
â”‚ â€¢ Handle multiple date columns         â”‚ â€¢ 2.48M records                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            AGGREGATION GENERATION                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ generate_unified_parquet_data.py       â”‚ regenerate_optimized_files.py        â”‚
â”‚ â€¢ Generate all aggregations            â”‚ â€¢ Regenerate optimized files          â”‚
â”‚ â€¢ All-time, yearly, quarterly          â”‚ â€¢ facts_awards_all_time.parquet      â”‚
â”‚ â€¢ 225+ parquet files total             â”‚ â€¢ facts_awards_title_optimized.parquetâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              FINAL PARQUET FILES                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ data/parquet/                                                                  â”‚
â”‚ â”œâ”€â”€ agg_*.parquet (4 files)           â”‚ â”œâ”€â”€ yearly/ (45 files)                â”‚
â”‚ â”œâ”€â”€ facts_awards_*.parquet (3 files)  â”‚ â””â”€â”€ quarterly/ (180 files)            â”‚
â”‚ â””â”€â”€ global_totals.json (1 file)       â”‚                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            API & DASHBOARD USAGE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Django API (backend/)                  â”‚ React Dashboard (frontend/)           â”‚
â”‚ â”œâ”€â”€ parquet_service.py                 â”‚ â”œâ”€â”€ Analytics Components             â”‚
â”‚ â”œâ”€â”€ parquet_search.py                  â”‚ â”œâ”€â”€ Search Components                â”‚
â”‚ â””â”€â”€ views.py                          â”‚ â””â”€â”€ Data Services                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—‚ï¸ **Raw Data Sources**

### **2013-2020 XLSX Files**
- **Location**: `data/raw/PHILGEPS 2013-2021/`
- **Format**: Excel (.xlsx) files
- **Structure**: Quarterly files with 44 columns
- **Volume**: ~8.9M records
- **Processing**: `process_xlsx_to_parquet_duckdb_basic_only.py`

### **2021-2025 CSV Files**
- **Location**: `data/raw/PHILGEPS -- 2021-2025 (CSV)/`
- **Format**: CSV files (one per year)
- **Structure**: 47 columns (3 additional columns)
- **Volume**: ~2.2M records
- **Processing**: `rebuild_step_by_step.py`

### **Flood Control Data**
- **Location**: `data/raw/SSP/`
- **Format**: Various formats
- **Purpose**: Specialized flood control contracts
- **Processing**: Integrated into main pipeline

## ğŸ”„ **Data Processing Pipeline**

### **Stage 1: Initial Processing**

#### **Script**: `process_xlsx_to_parquet_duckdb_basic_only.py`
- **Purpose**: Convert XLSX files to standardized Parquet format
- **Input**: Raw XLSX files (2013-2020)
- **Output**: Quarterly Parquet files in `data/processed/`
- **Features**:
  - Header standardization (row 3)
  - Column mapping to 44 standard columns
  - Deduplication (exact duplicates only)
  - Data cleaning and validation
  - Unique reference ID generation

#### **Script**: `rebuild_step_by_step.py`
- **Purpose**: Complete data rebuild and consolidation
- **Input**: Raw XLSX files + CSV files
- **Output**: `all_contracts_consolidated.parquet`
- **Process**:
  1. **Step 1**: Process 2013-2020 XLSX data
  2. **Step 2**: Process 2021-2025 CSV data
  3. **Step 3**: Combine both datasets
  4. **Step 4**: Cleanup temporary files

### **Stage 2: Data Quality & Formatting**

#### **Script**: `fix_date_formats.py`
- **Purpose**: Fix Excel serial date formatting issues
- **Input**: `all_contracts_consolidated.parquet`
- **Output**: `all_contracts_consolidated.parquet` (fixed)
- **Features**:
  - Convert Excel serial dates to proper date format
  - Handle multiple date columns
  - Preserve data integrity

#### **Script**: `generate_clean_awarded_contracts.py`
- **Purpose**: Generate clean awarded contracts dataset
- **Input**: `all_contracts_consolidated.parquet`
- **Output**: `clean_awarded_contracts_complete.parquet`
- **Filter**: Only contracts with `contract_amount > 0`
- **Purpose**: Master file for aggregation generation

### **Stage 3: Aggregation Generation**

#### **Script**: `generate_unified_parquet_data.py`
- **Purpose**: Generate all aggregation and facts files
- **Input**: `clean_awarded_contracts_complete.parquet`
- **Output**: Complete parquet file structure
- **Generates**:
  - **All-time aggregations**: `agg_*.parquet` files
  - **Yearly aggregations**: `data/parquet/yearly/year_YYYY/`
  - **Quarterly aggregations**: `data/parquet/quarterly/year_YYYY_qN/`
  - **Facts files**: `facts_awards_*.parquet`

#### **Script**: `regenerate_optimized_files.py`
- **Purpose**: Regenerate optimized files for dashboard
- **Input**: `all_contracts_consolidated.parquet`
- **Output**: Optimized parquet files
- **Generates**:
  - `facts_awards_all_time.parquet`
  - `facts_awards_title_optimized.parquet`

### **Stage 4: Maintenance & Cleanup**

#### **Script**: `cleanup_quarterly_raw_files.py`
- **Purpose**: Clean up redundant quarterly raw files
- **Input**: Quarterly files in `data/processed/`
- **Output**: Archived files in backup directory
- **Purpose**: Free up disk space (1.2GB+)

## ğŸ“ **File Structure & Outputs**

### **Consolidated Data Files**
```
data/processed/
â”œâ”€â”€ all_contracts_consolidated.parquet          # Main consolidated file (6.5M records, 50 columns)
â”œâ”€â”€ clean_awarded_contracts_complete.parquet    # Clean awarded contracts (2.48M records)
â””â”€â”€ backup_quarterly_raw_*/                     # Archived quarterly files
```

### **Aggregation Files**
```
data/parquet/
â”œâ”€â”€ agg_area.parquet                            # Area aggregations
â”œâ”€â”€ agg_business_category.parquet               # Business category aggregations
â”œâ”€â”€ agg_contractor.parquet                      # Contractor aggregations
â”œâ”€â”€ agg_organization.parquet                    # Organization aggregations
â”œâ”€â”€ facts_awards_all_time.parquet               # All-time facts
â”œâ”€â”€ facts_awards_title_optimized.parquet        # Optimized for search
â”œâ”€â”€ facts_awards_flood_control.parquet          # Flood control facts
â”œâ”€â”€ yearly/                                     # Yearly aggregations (45 files)
â”‚   â””â”€â”€ year_YYYY/
â”‚       â”œâ”€â”€ agg_*.parquet
â”‚       â””â”€â”€ facts_awards_year_YYYY.parquet
â””â”€â”€ quarterly/                                  # Quarterly aggregations (180 files)
    â””â”€â”€ year_YYYY_qN/
        â”œâ”€â”€ agg_*.parquet
        â””â”€â”€ facts_awards_year_YYYY_qN.parquet
```

### **Generated Files**
```
data/generated/
â””â”€â”€ global_totals.json                          # Global totals for percentages
```

## ğŸ”— **Codebase Usage**

### **Django API Usage**

#### **Parquet Service** (`backend/django/data_processing/parquet_service.py`)
- **Uses**: `agg_*.parquet` files, yearly/quarterly aggregations
- **Purpose**: Entity pagination and filtering
- **Endpoints**: `/api/analytics/entities/`

#### **Search Service** (`backend/django/contracts/parquet_search.py`)
- **Uses**: `facts_awards_title_optimized.parquet`, `facts_awards_all_time.parquet`
- **Purpose**: Full-text search functionality
- **Endpoints**: `/api/contracts/search/`

#### **Views** (`backend/django/philgeps_data_explorer/views.py`)
- **Uses**: All parquet files
- **Purpose**: Serve parquet files to frontend
- **Endpoints**: `/api/parquet/`

### **Frontend Usage**

#### **Analytics Components**
- **QuarterlyTrendsChart**: Uses aggregation data for trend visualization
- **AnalyticsControls**: Uses yearly/quarterly data for filtering
- **DataExplorer**: Uses all data for exploration interface

#### **Search Components**
- **AdvancedSearch**: Uses optimized facts files for search
- **EntityDrillDownModal**: Uses complete dataset for drill-down analysis

#### **Data Services**
- **AdvancedSearchService**: Calls Django API for search functionality
- **AnalyticsService**: Calls Django API for analytics data

## ğŸ”„ **Complete Data Flow**

### **1. Raw Data Ingestion**
```
XLSX Files (2013-2020) â†’ process_xlsx_to_parquet_duckdb_basic_only.py â†’ Quarterly Parquet Files
CSV Files (2021-2025) â†’ rebuild_step_by_step.py â†’ Consolidated Data
```

### **2. Data Consolidation**
```
Quarterly Files + CSV Data â†’ rebuild_step_by_step.py â†’ all_contracts_consolidated.parquet
```

### **3. Data Quality & Cleaning**
```
Consolidated Data â†’ fix_date_formats.py â†’ Fixed Date Formats
Consolidated Data â†’ generate_clean_awarded_contracts.py â†’ Clean Awarded Contracts
```

### **4. Aggregation Generation**
```
Clean Data â†’ generate_unified_parquet_data.py â†’ All Aggregation Files
Clean Data â†’ regenerate_optimized_files.py â†’ Optimized Files
```

### **5. API & Dashboard Usage**
```
Aggregation Files â†’ Django API â†’ React Dashboard
Facts Files â†’ Search API â†’ Search Interface
```

## ğŸ› ï¸ **Script Dependencies**

### **Core Dependencies**
- **DuckDB**: All data processing scripts
- **Pandas**: Data analysis and manipulation
- **Python 3.13**: Runtime environment

### **Script Execution Order**
1. `process_xlsx_to_parquet_duckdb_basic_only.py` (if processing XLSX)
2. `rebuild_step_by_step.py` (complete rebuild)
3. `fix_date_formats.py` (date formatting)
4. `generate_clean_awarded_contracts.py` (clean data)
5. `generate_unified_parquet_data.py` (aggregations)
6. `regenerate_optimized_files.py` (optimized files)
7. `cleanup_quarterly_raw_files.py` (cleanup)

## ğŸ“Š **Data Volume & Performance**

### **File Sizes**
- **Consolidated**: ~2.8GB (6.5M records, 50 columns)
- **Clean Awarded**: ~1.1GB (2.48M records)
- **Aggregations**: ~500MB total
- **Optimized Files**: ~200MB total

### **Processing Times**
- **XLSX Processing**: ~30-60 minutes
- **CSV Processing**: ~10-15 minutes
- **Aggregation Generation**: ~20-30 minutes
- **Optimization**: ~5-10 minutes

## ğŸ”§ **Configuration**

### **Core Configuration** (`scripts/core/config.py`)
- **Years**: 2013-2025
- **Quarters**: 1-4
- **Chunk Size**: 200 records
- **File Paths**: Centralized configuration

### **Environment Variables**
- **Data Source**: `data/processed/clean_awarded_contracts_complete.parquet`
- **Output Directory**: `data/parquet/`
- **Log Directory**: `logs/`

## ğŸš€ **Usage Examples**

### **Complete Rebuild**
```bash
# 1. Rebuild complete dataset
python scripts/rebuild_step_by_step.py

# 2. Fix date formats
python scripts/fix_date_formats.py

# 3. Generate clean awarded contracts
python scripts/generate_clean_awarded_contracts.py

# 4. Generate all aggregations
python scripts/core/generate_unified_parquet_data.py

# 5. Regenerate optimized files
python scripts/regenerate_optimized_files.py

# 6. Cleanup (optional)
python scripts/cleanup_quarterly_raw_files.py --confirm
```

### **Maintenance Tasks**
```bash
# Regenerate aggregations only
python scripts/core/generate_unified_parquet_data.py

# Regenerate optimized files only
python scripts/regenerate_optimized_files.py

# Cleanup old files
python scripts/cleanup_quarterly_raw_files.py --confirm
```

## ğŸ“ **Notes**

- **Data Integrity**: All scripts include backup creation before processing
- **Error Handling**: Comprehensive error handling and logging
- **Memory Management**: Chunked processing for large datasets
- **Validation**: Data quality checks at each stage
- **Documentation**: Each script includes comprehensive documentation

---

**Last Updated**: January 2, 2025  
**Version**: v1.0 - Complete Data Pipeline Documentation
